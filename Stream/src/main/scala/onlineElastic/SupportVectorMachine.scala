package onlineElastic

import org.apache.spark.ml.classification.LinearSVC
import org.apache.spark.ml.feature.PCA
import org.apache.spark.sql.functions._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._

import br.ufrj.gta.stream.metrics._
import br.ufrj.gta.stream.schema.flow.Flowtbag
import br.ufrj.gta.stream.util.File

import org.elasticsearch.spark._
import org.elasticsearch.spark.sql._
import org.elasticsearch.hadoop.cfg.ConfigurationOptions

object SupportVectorMachineElastic {
    def main(args: Array[String]) {

        // Sets dataset file separation string ("," for csv) and label column name
        val sep = ","
        val labelCol = "label"

        // Sets names for the colum created containing regular features
        var featuresCol = "features"

        // Defines dataset schema, dataset csv generated by flowtbag https://github.com/DanielArndt/flowtbag 
        val schema = Flowtbag.getSchema

        // Creates spark session
        val spark = SparkSession
            .builder
            .config(ConfigurationOptions.ES_NODES, "localhost")
            .config(ConfigurationOptions.ES_PORT, "9200")
            .config(ConfigurationOptions.ES_RESOURCE, "spark/classification")
            .appName("Stream")
            .getOrCreate()

        // Checks arguments
        if (args.length < 3) {
            println("Missing parameters")
            sys.exit(1)
        }

        // Path for training dataset file
        val inputTrainingFile = args(0)

        // Kafka server address
        val kafkaServer = args(1)

        // Kafka topic name
        val flowsTopic = args(2)

        // Path and filenames for saving progress and classification results
        val outputPath = File.appendSlash(args(3))

        // Sets algorithm hyperparameters
        val regParam = 0.01
        val maxIter = 20

        // Reads csv dataset file, fitting it to the schema
        val inputTrainingData = spark.read
            .option("sep", sep)
            .option("header", false)
            .schema(schema)
            .csv(inputTrainingFile)

        // Reads new data arriving at the Kafka topic
        val inputTestDataStream = spark.readStream
            .format("kafka")
            .option("kafka.bootstrap.servers", kafkaServer)
            .option("subscribe", flowsTopic)
            .load()

        // Creates a single vector column containing all features on the training data
        val trainingData = Flowtbag.featurize(inputTrainingData, featuresCol)

        // Receives flow value data from the Kafka topic, and converts it to String
        val valueDataStream = inputTestDataStream
            .select(inputTestDataStream("value").cast("string"))

        // Creates a DataFrame containing the network flow data, adapting the data to fit the Flowtbag format
        val flowsDataStream = valueDataStream
            .withColumn("fields", split(regexp_replace(valueDataStream("value"), "\"", ""), ","))
            .select(Flowtbag.getColsRange.map(c => col("fields").getItem(c).as(s"col$c").cast("int")): _*)
            .toDF(Flowtbag.getColNames: _*)

        // Creates a single vector column containing all features on the test data
        val testData = Flowtbag.featurize(flowsDataStream, featuresCol)

        // Creates a Support Vector Machine classifier, using the hyperparameters defined previously
        val classifier = new LinearSVC()
            .setFeaturesCol(featuresCol)
            .setLabelCol(labelCol)
            .setRegParam(regParam)
            .setMaxIter(maxIter)

        // Fits the training data to the classifier, creating the classification model
        val model = classifier.fit(trainingData)

        // Tests model on the test data
        val prediction = model.transform(testData)

        // Creates a column with the classification prediction
        val predictionCol = classifier.getPredictionCol

        // Write the classification results on Elasticsearch
        val outputDataStream = prediction
            .select(prediction("srcip"), prediction("srcport"), prediction("dstip"), prediction("dstport"), prediction("proto"), prediction(labelCol), prediction(predictionCol))
            .writeStream
            .format("es")
            .option("checkpointLocation", outputPath)
            .start("spark/classification")

        // Wait until timeout to stop the online classification tool
        outputDataStream.awaitTermination()

        spark.stop()
    }
}
