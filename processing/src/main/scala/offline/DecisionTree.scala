package offline

import org.apache.spark.ml.classification.DecisionTreeClassifier
import org.apache.spark.ml.feature.PCA
import org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator, BinaryClassificationEvaluator}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._
import org.apache.spark.sql.Row

import br.ufrj.gta.stream.metrics._
import br.ufrj.gta.stream.schema.flow.Flowtbag
import br.ufrj.gta.stream.util.File

import org.elasticsearch.spark._
import org.elasticsearch.spark.sql._
import org.elasticsearch.hadoop.cfg.ConfigurationOptions

object DecisionTree {
    def main(args: Array[String]) {

        // Sets dataset file separation string ("," for csv) and label column name
        val sep = ","
        val labelCol = "label"

        // Sets names for colums created during the algorithm execution, containing PCA and regular features
        val pcaFeaturesCol = "pcaFeatures"
        var featuresCol = "features"

        // Defines dataset schema, dataset csv generated by flowtbag https://github.com/DanielArndt/flowtbag
        val schema = Flowtbag.getSchema

        // Checks arguments
        if (args.length < 2) {
            println("Missing parameters")
            sys.exit(1)
        }

        // Path for dataset used from training and testing
        val inputFile = args(0)

        // String cointaining all slave nodes; defaults to localhost if empty
        val slaveNodes = if (args(1) != "") args(1) else "localhost"

        // Dataset used for model creation and test
        val dataset = args(2)

        // Sets algorithm hyperparameters
        //val impurity = args(2)
        //val maxDepth = args(3).toInt
        val impurity = "gini"
        val maxDepth = 20

        // Creates spark session
        val spark = SparkSession
            .builder
            .config(ConfigurationOptions.ES_NODES, slaveNodes)
            .config(ConfigurationOptions.ES_RESOURCE, "spark-offline/classification")
            .appName("Stream")
            .getOrCreate()

        // Reads csv dataset file, fitting it to the schema
        val inputData = spark.read
            .option("sep", sep)
            .option("header", false)
            .schema(schema)
            .csv(inputFile)

        // Creates a single vector column containing all features
        val featurizedData = Flowtbag.featurize(inputData, featuresCol)

        // Splits the dataset on training (70%) and test (30%)
        val splitData = featurizedData.randomSplit(Array(0.7, 0.3))
        val trainingData = splitData(0)
        val testData = splitData(1)

        var startTime = System.currentTimeMillis()

        // Creates a Decision Tree classifier, using the hyperparameters defined previously
        val classifier = new DecisionTreeClassifier()
            .setFeaturesCol(featuresCol)
            .setLabelCol(labelCol)
            .setMaxDepth(maxDepth)
            .setImpurity(impurity)

        // Fits the training data to the classifier, creating the classification model
        val model = classifier.fit(trainingData)

        val trainingTime = (System.currentTimeMillis() - startTime) / 1000.0

        startTime = System.currentTimeMillis()

        // Tests model on the test data
        val prediction = model.transform(testData)

        // Creates a column with the classification prediction
        val predictionCol = classifier.getPredictionCol

        // Cache model to improve performance
        prediction.cache()

        // Performs an action to accurately measure the test time
        prediction.count()

        val testTime = (System.currentTimeMillis() - startTime) / 1000.0

        // Removes model from cache
        prediction.unpersist()

        // Compute evaluation metrics
        val f1Evaluator = new MulticlassClassificationEvaluator().setMetricName("f1")
        val weightedPrecisionEvaluator = new MulticlassClassificationEvaluator().setMetricName("weightedPrecision")
        val weightedRecallEvaluator = new MulticlassClassificationEvaluator().setMetricName("weightedRecall")
        val accuracyEvaluator = new MulticlassClassificationEvaluator().setMetricName("accuracy")
        val aucEvaluator = new BinaryClassificationEvaluator().setMetricName("areaUnderROC")

        val f1 = f1Evaluator.evaluate(prediction)
        val weightedPrecision = weightedPrecisionEvaluator.evaluate(prediction)
        val weightedRecall = weightedRecallEvaluator.evaluate(prediction)
        val accuracy = accuracyEvaluator.evaluate(prediction)
        val auc = aucEvaluator.evaluate(prediction)

        // Creates a DataFrame with the resulting metrics, and send them to ElasticSearch
        val elasticDF = Seq(Row("Decision Tree", accuracy, weightedPrecision, weightedRecall, f1, auc, trainingTime, dataset))
        val elasticSchema = List(
          StructField("algorithm", StringType, true),
          StructField("accuracy", DoubleType, true),
          StructField("precision", DoubleType, true),
          StructField("recall", DoubleType, true),
          StructField("f1-score", DoubleType, true),
          StructField("auc", DoubleType, true),
          StructField("training time", DoubleType, true),
          StructField("dataset", StringType, true))
        val someDF = spark
            .createDataFrame(spark.sparkContext.parallelize(elasticDF),StructType(elasticSchema))
            .saveToEs("spark-offline/classification")

        spark.stop()
    }
}