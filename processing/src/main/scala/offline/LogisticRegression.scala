package offline

import org.apache.spark.ml.classification.{LogisticRegression => LogisticRegressionClassifier}
import org.apache.spark.ml.feature.PCA
import org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator, BinaryClassificationEvaluator}
import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}
import org.apache.spark.mllib.evaluation.MulticlassMetrics
import org.apache.spark.sql.functions._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.types._
import org.apache.spark.sql.Row

import br.ufrj.gta.stream.metrics._
import br.ufrj.gta.stream.schema.flow.Flowtbag
import br.ufrj.gta.stream.util.File

import org.elasticsearch.spark._
import org.elasticsearch.spark.sql._
import org.elasticsearch.hadoop.cfg.ConfigurationOptions

object LogisticRegression {
    def main(args: Array[String]) {

        // Sets dataset file separation string ("," for csv) and label column name
        val sep = ","
        val labelCol = "label"

        // Sets names for colums created during the algorithm execution, containing PCA and regular features
        val pcaFeaturesCol = "pcaFeatures"
        var featuresCol = "features"

        // Defines dataset schema, dataset csv generated by flowtbag https://github.com/DanielArndt/flowtbag
        val schema = Flowtbag.getSchema

        // Checks arguments
        if (args.length < 2) {
            println("Missing parameters")
            sys.exit(1)
        }

        // Path for dataset used from training and testing
        val inputFile = args(0)

        // String cointaining all slave nodes; defaults to localhost if empty
        val slaveNodes = if (args(1) != "local") args(1) else "localhost"

        // Dataset used for model creation and test
        val dataset = args(2)

        // Creates spark session
        val spark = SparkSession
            .builder
            .config(ConfigurationOptions.ES_NODES, slaveNodes)
            .config(ConfigurationOptions.ES_RESOURCE, "spark-offline/classification")
            .appName("Stream")
            .getOrCreate()

        // Reads csv dataset file, fitting it to the schema
        val inputData = spark.read
            .option("sep", sep)
            .option("header", false)
            .schema(schema)
            .csv(inputFile)

        // Creates a single vector column containing all features
        val featurizedData = Flowtbag.featurize(inputData, featuresCol)

        // Splits the dataset on training (70%) and test (30%), with the seed '534661'
        val splitData = featurizedData.randomSplit(Array(0.7, 0.3), 534661)
        val trainingData = splitData(0)
        val testData = splitData(1)

        var startTime = System.currentTimeMillis()

        // Creates a Logistic Regression classifier, using the hyperparameters defined previously
        val lr = new LogisticRegressionClassifier()
           .setFeaturesCol(featuresCol)
           .setLabelCol(labelCol)

        val paramGrid = new ParamGridBuilder()
            //.addGrid(lr.elasticNetParam, Array(0.0,0.25,0.5,0.75,1.0))    // Param for the ElasticNet mixing parameter
            //.addGrid(lr.fitIntercept, Array(true, false))                 // Param for whether to fit an intercept term
            //.addGrid(lr.maxIter, Array(5,10,20,50,100,200))               // Param for maximum number of iterations
            //.addGrid(lr.regParam, Array(0.0,0.01,0.1,0.3))                // Param for regularization parameter
            //.addGrid(lr.standardization, Array(true, false))              // Param for whether to standardize the training features before fitting the model
            //.addGrid(lr.tol, Array(0.000001,0.00001,0.0001,0.001))        // Param for the convergence tolerance for iterative algorithms
            .build()

        val evaluator = new MulticlassClassificationEvaluator
        //evaluator.setMetricName("weightedPrecision")                      // Uncomment this line to make the evaluator prioritize another metric

        val classifier = new CrossValidator()
            .setEstimator(lr)
            .setEstimatorParamMaps(paramGrid)
            .setEvaluator(evaluator)
            .setNumFolds(10)
            .setSeed(534661)

        // Fits the training data to the classifier, creating the classification model
        val model = classifier.fit(trainingData)

        val hyperparameters = model.bestModel.extractParamMap()

        val trainingTime = (System.currentTimeMillis() - startTime) / 1000.0

        startTime = System.currentTimeMillis()

        // Tests model on the test data
        val prediction = model.transform(testData)

        // Cache model to improve performance
        prediction.cache()

        // Perform an action to accurately measure the test time
        prediction.count()

        val testTime = (System.currentTimeMillis() - startTime) / 1000.0

        // Removes model from cache
        prediction.unpersist()

        // Compute evaluation metrics
        import spark.implicits._
        val predictionAndLabel = prediction.select("prediction","label").as[(Double, Double)].rdd
        val metrics = new MulticlassMetrics(predictionAndLabel)
        val accuracy = metrics.accuracy
        val precision = metrics.precision(1)
        val recall = metrics.recall(1)
        val f1 = metrics.fMeasure(1)

        val aucEvaluator = new BinaryClassificationEvaluator().setMetricName("areaUnderROC")
        val auc = aucEvaluator.evaluate(prediction)

        // Creates a DataFrame with the resulting metrics, and send them to ElasticSearch
        val elasticDF = Seq(Row("Logistic Regression", accuracy, precision, recall, f1, auc, trainingTime, dataset, hyperparameters.toString()))
        val elasticSchema = List(
          StructField("algorithm", StringType, true),
          StructField("accuracy", DoubleType, true),
          StructField("precision", DoubleType, true),
          StructField("recall", DoubleType, true),
          StructField("f1-score", DoubleType, true),
          StructField("auc", DoubleType, true),
          StructField("training time", DoubleType, true),
          StructField("dataset", StringType, true),
          StructField("hyperparameters", StringType, true))
        val someDF = spark
            .createDataFrame(spark.sparkContext.parallelize(elasticDF),StructType(elasticSchema))
            .saveToEs("spark-offline/classification")

        spark.stop()
    }
}