package offline

import org.apache.spark.ml.feature.PCA
import org.apache.spark.sql.functions._
import org.apache.spark.sql.SparkSession

import br.ufrj.gta.stream.metrics._
import br.ufrj.gta.stream.ml.classification.anomaly.MeanVarianceClassifier
import br.ufrj.gta.stream.schema.flow.Flowtbag
import br.ufrj.gta.stream.util.File

object MeanVariance {
    def main(args: Array[String]) {
        
        // Sets dataset file separation string ("," for csv) and label column name
        val sep = ","
        val labelCol = "label"

        // Sets names for colums created during the algorithm execution, containing PCA and regular features
        val pcaFeaturesCol = "pcaFeatures"
        var featuresCol = "features"

        // Defines dataset schema, dataset csv generated by flowtbag https://github.com/DanielArndt/flowtbag 
        val schema = Flowtbag.getSchema

        // Creates spark session
        val spark = SparkSession.builder.appName("Stream").getOrCreate()

        // Checks arguments
        if (args.length < 6) {
            println("Missing parameters")
            sys.exit(1)
        }

        // Path for training dataset file (assumes all entries to contain legitimate traffic)
        val inputTrainingFile = args(0)

        // Path for test dataset file
        val inputTestFile = args(1)

        // Filename for saving classification results
        val metricsFilename = args(2)

        // Number of the algorithm is executed; used to obtain the Confidence Interval
        val numSims = args(3).toInt

        // Number of Cores used on each Slave; used to test the impact of parallelism
        val numCores = args(4).toInt

        // Sets algorithm hyperparameters
        val threshold = args(5).toDouble

        // Sets total number of PCA features; optional
        val pcaK: Option[Int] = try {
            Some(args(6).toInt)
        } catch {
            case e: Exception => None
        }

        // Reads the training csv dataset file, fitting it to the schema
        val inputTrainingData = spark.read
            .option("sep", sep)
            .option("header", false)
            .schema(schema)
            .csv(inputTrainingFile)

        // Reads the test csv dataset file, fitting it to the schema
        val inputTestData = spark.read
            .option("sep", sep)
            .option("header", false)
            .schema(schema)
            .csv(inputTestFile)

        // Creates a single vector column containing all features on training and test data
        val featurizedTrainingData = Flowtbag.featurize(inputTrainingData, featuresCol)
        val featurizedTestData = Flowtbag.featurize(inputTestData, featuresCol)

        // Defines all default metrics, and adds 3 extra ones
        val metrics = new PredictionMetrics(PredictionMetrics.names ++ Array("Number of cores", "Training time", "Test time"))

        for (i <- 0 until numSims) {

            // Splits the training dataset on two subsections; larger section (70%) is selected as new training dataset
            val splitData = Array(featurizedTrainingData.randomSplit(Array(0.7, 0.3))(0), featurizedTestData.randomSplit(Array(0.7, 0.3))(1))

            var startTime = System.currentTimeMillis()

            // Applies PCA to training and test data; optional
            val (trainingData, testData) = pcaK match {
                case Some(pcaK) => {
                    val pca = new PCA()
                        .setInputCol(featuresCol)
                        .setOutputCol(pcaFeaturesCol)
                        .setK(pcaK)
                        .fit(splitData(0))

                    featuresCol = pcaFeaturesCol

                    (pca.transform(splitData(0)), pca.transform(splitData(1)))
                }
                case None => (splitData(0), splitData(1))
            }

            // Creates a Mean-Variance classifier, using the hyperparameters defined previously
            val classifier = new MeanVarianceClassifier()
                .setFeaturesCol(featuresCol)
                .setLabelCol(labelCol)
                .setThreshold(threshold)

            // Fits the training data to the classifier, creating the classification model
            val model = classifier.fit(trainingData)

            val trainingTime = (System.currentTimeMillis() - startTime) / 1000.0

            startTime = System.currentTimeMillis()

            // Tests model on the test data
            val prediction = model.transform(testData)

            // Creates a column with the classification prediction
            val predictionCol = classifier.getPredictionCol

            // Cache model to improve performance
            prediction.cache()

            // Perform an action to accurately measure the test time
            prediction.count()

            val testTime = (System.currentTimeMillis() - startTime) / 1000.0

            // Obtains metrics using the label and metrics columns
            metrics.add(metrics.getMetrics(prediction, labelCol, predictionCol) + ("Number of cores" -> numCores, "Training time" -> trainingTime, "Test time" -> testTime))

            // Removes movel from cache since the iteration is over
            prediction.unpersist()
        }

        // Saves metric results on a csv file
        metrics.export(metricsFilename, Metrics.FormatCsv)

        spark.stop()
    }
}
