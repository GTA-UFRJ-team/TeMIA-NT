import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.Row
import org.apache.spark.sql.SparkSession

import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, VectorIndexer, IndexToString, PCA}
import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}
import org.apache.spark.ml.linalg.{Vector, Vectors}
import org.apache.spark.ml.stat.Summarizer

import org.apache.spark.mllib.linalg.Matrix
import org.apache.spark.mllib.linalg.distributed.RowMatrix

import org.apache.spark.rdd.RDD

import better.files._
import better.files.File._

object scalaAnomaly {

        def main(args: Array[String]) = {

                // Check arguments
                if(args.length < 2 ) {
                        println("Usage: scala.scl <csv dataset> threshold")
                        sys.exit(1)
                }

                val csv_treino = args(0)
		val csv_teste = args(1)

                // Create spark session
                val spark = SparkSession
			.builder()
                        .appName("Catraca")
                        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
                        .master("spark://master:7077")
                        .getOrCreate

		import spark.implicits._

                // Define dataset schema, dataset csv generated by flowtbag https://github.com/DanielArndt/flowtbag
                val schema = StructType(
                        StructField("srcip", StringType, false) ::              //Feature 1
                        StructField("srcport", IntegerType, false) ::           //Feature 2
                        StructField("dstip", StringType, false) ::              //Feature 3
                        StructField("dstport", IntegerType, false) ::           //Feature 4
                        StructField("proto", IntegerType, false) ::             //Feature 5
                        StructField("total_fpackets", IntegerType, false) ::    //Feature 6
                        StructField("total_fvolume", IntegerType, false) ::     //Feature 7
                        StructField("total_bpackets", IntegerType, false) ::    //Feature 8
                        StructField("total_bvolume", IntegerType, false) ::     //Feature 9
                        StructField("min_fpktl", IntegerType, false) ::         //Feature 10
                        StructField("mean_fpktl", IntegerType, false) ::        //Feature 11
                        StructField("max_fpktl", IntegerType, false) ::         //Feature 12
                        StructField("std_fpktl", IntegerType, false) ::         //Feature 13
                        StructField("min_bpktl", IntegerType, false) ::         //Feature 14
                        StructField("mean_bpktl", IntegerType, false) ::        //Feature 15
                        StructField("max_bpktl", IntegerType, false) ::         //Feature 16
                        StructField("std_bpktl", IntegerType, false) ::         //Feature 17
                        StructField("min_fiat", IntegerType, false) ::          //Feature 18
                        StructField("mean_fiat", IntegerType, false) ::         //Feature 19
                        StructField("max_fiat", IntegerType, false) ::          //Feature 20
                        StructField("std_fiat", IntegerType, false) ::          //Feature 21
                        StructField("min_biat", IntegerType, false) ::          //Feature 22
                        StructField("mean_biat", IntegerType, false) ::         //Feature 23
                        StructField("max_biat", IntegerType, false) ::          //Feature 24
                        StructField("std_biat", IntegerType, false) ::          //Feature 25
                        StructField("duration", IntegerType, false) ::          //Feature 26
                        StructField("min_active", IntegerType, false) ::        //Feature 27
                        StructField("mean_active", IntegerType, false) ::       //Feature 28
                        StructField("max_active", IntegerType, false) ::        //Feature 29
                        StructField("std_active", IntegerType, false) ::        //Feature 30
                        StructField("min_idle", IntegerType, false) ::          //Feature 31
                        StructField("mean_idle", IntegerType, false) ::         //Feature 32
                        StructField("max_idle", IntegerType, false) ::          //Feature 33
                        StructField("std_idle", IntegerType, false) ::          //Feature 34
                        StructField("sflow_fpackets", IntegerType, false) ::    //Feature 35
                        StructField("sflow_fbytes", IntegerType, false) ::      //Feature 36
                        StructField("sflow_bpackets", IntegerType, false) ::    //Feature 37
                        StructField("sflow_bbytes", IntegerType, false) ::      //Feature 38
                        StructField("fpsh_cnt", IntegerType, false) ::          //Feature 39
                        StructField("bpsh_cnt", IntegerType, false) ::          //Feature 40
                        StructField("furg_cnt", IntegerType, false) ::          //Feature 41
                        StructField("burg_cnt", IntegerType, false) ::          //Feature 42
                        StructField("total_fhlen", IntegerType, false) ::       //Feature 43
                        StructField("total_bhlen", IntegerType, false) ::       //Feature 44
                        StructField("dscp", IntegerType, false) ::              //Feature 45
                        StructField("label", IntegerType, false) ::             //Class Label: 0-Normal; 1-Attack
                        Nil)

                // Load CSV data
                val trainCsv = spark.read.format("csv").schema(schema).load(csv_treino)
		val testCsv = spark.read.format("csv").schema(schema).load(csv_teste)

                // Create vector assembler to produce a feature vector for each record for use in MLlib
                // First 45 csv fields are features, the 46th field is the label. Remove IPs from features.
                val assembler = new VectorAssembler()
                        .setInputCols((schema.fieldNames.toList.takeRight(41).dropRight(1)).toArray)
                        .setOutputCol("features")

                // Assemble feature vector in new dataframe
                val assembledTrainData = assembler.transform(trainCsv)
		val assembledTestData = assembler.transform(testCsv)

                val Array(trainingData, testData) = Array(assembledTrainData, assembledTestData)

		val (meanVal, varianceVal) = trainingData.select(Summarizer.metrics("mean", "variance")
			.summary($"features").as("summary"))
			.select("summary.mean", "summary.variance")
			.as[(Vector, Vector)].first()

		val arrayMean = meanVal.toDense.toArray
		val arrayVariance = varianceVal.toDense.toArray
		
		val baseModelPlus = arrayMean.zip(arrayVariance).map { case (x,y) => x + y*args(2).toFloat }
		val baseModelMinus = arrayMean.zip(arrayVariance).map { case (x,y) => x - y*args(2).toFloat }

		println(baseModelPlus.deep.mkString(","))
		println(baseModelMinus.deep.mkString(","))

		def filterAtaque(r: Row, baseModelMinus: Array[Double], baseModelPlus: Array[Double]): Boolean = {
			for (a <- 0 to 39) {
				if (r.getInt(a+5) < baseModelMinus(a) || r.getInt(a+5) > baseModelPlus(a)) {
					return true // ataque
				}
			}

			return false // legitimo
		}		

		def filterLegitimo(r: Row, baseModelMinus: Array[Double], baseModelPlus: Array[Double]): Boolean = {
			for (a <- 0 to 39) {
				if (r.getInt(a+5) < baseModelMinus(a) || r.getInt(a+5) > baseModelPlus(a)) {
					return false // ataque
                                        }
                                }

                        return true // legitimo
                }			

		val temp = testData.rdd.collect().filter(r => filterLegitimo(r, baseModelMinus, baseModelPlus))
		val temp2 = testData.rdd.collect().filter(r => filterAtaque(r, baseModelMinus, baseModelPlus))
		println(temp.length)
		println(temp2.length)

                //Stop spark session
                val input = readLine("prompt> ")
		spark.stop()

	}
}
